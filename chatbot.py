import os
from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# Load environment variables from .env file
load_dotenv()

loader = PyPDFLoader("data/meditations.pdf")
"""
Loads the PDF document and returns a list of documents.
"""
docs = loader.load()

# Get OpenAI API key from environment
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables. Please add it to your .env file.")

embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
vectordb = Chroma.from_documents(docs, embedding=embeddings, persist_directory="db")
vectordb.persist()


# chatbot.py - RAG Chain
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo", openai_api_key=openai_api_key)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True
)

def ask_question(query):
    """
    Answers a question using the RAG pipeline.
    Args:
        query (str): The user's question.
    Returns:
        str: The answer generated by the RAG pipeline.
    """
    response = qa_chain(query)
    return response["result"]
