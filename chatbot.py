import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import google.generativeai as genai

# Load environment variables from .env file
load_dotenv()

gemini_api_key = os.getenv("GEMINI_API_KEY")
if not gemini_api_key:
    raise ValueError("GEMINI_API_KEY not found in environment variables. Please add it to your .env file.")

genai.configure(api_key=gemini_api_key)

# Load and embed documents
loader = PyPDFLoader("data/meditations.pdf")
docs = loader.load()

# Use HuggingFaceEmbeddings for document embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectordb = Chroma.from_documents(docs, embedding=embeddings, persist_directory="db")
vectordb.persist()
retriever = vectordb.as_retriever()

def ask_question(query):
    """
    Answers a question using retrieved context and Gemini LLM.
    Args:
        query (str): The user's question.
    Returns:
        str: The answer generated by Gemini LLM.
    """
    # Retrieve relevant documents
    docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in docs])
    prompt = f"Answer the following question based on the context below.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:"
    model = genai.GenerativeModel("gemini-pro")
    response = model.generate_content(prompt)
    return response.text.strip()
